# Search-R1

- **WAIT WHAT? THIS ONLY USE 1 REWARD FUNCTION? ğŸ¤¯** (outcome-based reward function - Exactmatch)
- Still required the model to generate xml structured output, but does not have a reward function to check the format.
- [ ] Develop deepsearch further from this project. The code is very detailed and well-written.
- <https://github.com/PeterGriffinJin/Search-R1>
- <https://arxiv.org/pdf/2503.09516>
- Trained a 3B qwen model with GRPO and multi hop tool call ability
- Reproduce the paper: <https://github.com/PeterGriffinJin/Search-R1/tree/main/scripts/nq_hotpotqa>
- Apache-2.0 license

# Summary Key Points with NotebookLM

Dá»±a trÃªn cÃ¡c nguá»“n, SEARCH-R1 giá»›i thiá»‡u má»™t **khung há»c tÄƒng cÆ°á»ng (RL) má»›i cho phÃ©p cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) tá»± Ä‘á»™ng xen káº½ quÃ¡ trÃ¬nh suy luáº­n vá»›i tÆ°Æ¡ng tÃ¡c vá»›i cÃ´ng cá»¥ tÃ¬m kiáº¿m theo thá»i gian thá»±c**. Má»¥c tiÃªu chÃ­nh lÃ  giÃºp LLMs **thu tháº­p kiáº¿n thá»©c bÃªn ngoÃ i vÃ  thÃ´ng tin cáº­p nháº­t má»™t cÃ¡ch hiá»‡u quáº£** Ä‘á»ƒ nÃ¢ng cao kháº£ nÄƒng suy luáº­n vÃ  táº¡o vÄƒn báº£n cá»§a chÃºng.

- **Há»— trá»£ truy xuáº¥t vÃ  suy luáº­n nhiá»u lÆ°á»£t**, trong Ä‘Ã³ cÃ¡c lá»‡nh gá»i tÃ¬m kiáº¿m Ä‘Æ°á»£c kÃ­ch hoáº¡t rÃµ rÃ ng báº±ng cÃ¡c mÃ£ thÃ´ng bÃ¡o `<search>` vÃ  `</search>`, cÃ²n ná»™i dung Ä‘Æ°á»£c truy xuáº¥t Ä‘Æ°á»£c bao quanh bá»Ÿi cÃ¡c mÃ£ thÃ´ng bÃ¡o `<information>` vÃ  `</information>`, vÃ  cÃ¡c bÆ°á»›c suy luáº­n cá»§a LLM Ä‘Æ°á»£c bao quanh bá»Ÿi `<think>` vÃ  `</think>`, vá»›i cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng báº±ng `<answer>` vÃ  `</answer>`.
- **Ãp dá»¥ng ká»¹ thuáº­t che phá»§ mÃ£ thÃ´ng bÃ¡o Ä‘Æ°á»£c truy xuáº¥t (retrieved token masking)** Ä‘á»ƒ Ä‘áº£m báº£o **tá»‘i Æ°u hÃ³a RL á»•n Ä‘á»‹nh**, báº±ng cÃ¡ch chá»‰ tÃ­nh toÃ¡n má»¥c tiÃªu gradient chÃ­nh sÃ¡ch trÃªn cÃ¡c mÃ£ thÃ´ng bÃ¡o do LLM táº¡o ra vÃ  loáº¡i trá»« ná»™i dung Ä‘Æ°á»£c truy xuáº¥t khá»i quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a.
- Sá»­ dá»¥ng má»™t **hÃ m thÆ°á»Ÿng Ä‘Æ¡n giáº£n dá»±a trÃªn káº¿t quáº£ cuá»‘i cÃ¹ng (outcome-based reward function)**, Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng, cháº³ng háº¡n nhÆ° sá»­ dá»¥ng so khá»›p chuá»—i chÃ­nh xÃ¡c (Exact Match - EM) trong cÃ¡c tÃ¡c vá»¥ suy luáº­n dá»±a trÃªn Ñ„Ğ°ĞºÑ‚Ñ‹. HÃ m thÆ°á»Ÿng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  `rÏ•(x, y) = EM(apred, agold)`. Thiáº¿t káº¿ thÆ°á»Ÿng tá»‘i thiá»ƒu nÃ y Ä‘Æ°á»£c chá»©ng minh lÃ  hiá»‡u quáº£ trong cÃ¡c tÃ¬nh huá»‘ng tÃ¬m kiáº¿m vÃ  suy luáº­n.

**Vá» hÃ m thÆ°á»Ÿng vÃ  GRPO:**

- SEARCH-R1 sá»­ dá»¥ng má»™t **há»‡ thá»‘ng thÆ°á»Ÿng dá»±a trÃªn quy táº¯c chá»‰ bao gá»“m pháº§n thÆ°á»Ÿng káº¿t quáº£ cuá»‘i cÃ¹ng**. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  mÃ´ hÃ¬nh chá»‰ Ä‘Æ°á»£c thÆ°á»Ÿng dá»±a trÃªn viá»‡c cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng cá»§a nÃ³ cÃ³ Ä‘Ãºng hay khÃ´ng so vá»›i Ä‘Ã¡p Ã¡n thá»±c táº¿. CÃ¡c tÃ¡c giáº£ Ä‘Ã£ cá»‘ tÃ¬nh trÃ¡nh sá»­ dá»¥ng pháº§n thÆ°á»Ÿng Ä‘á»‹nh dáº¡ng phá»©c táº¡p hoáº·c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh thÆ°á»Ÿng tháº§n kinh (neural reward models) do lo ngáº¡i vá» viá»‡c "hack" pháº§n thÆ°á»Ÿng vÃ  chi phÃ­ tÃ­nh toÃ¡n cÅ©ng nhÆ° Ä‘á»™ phá»©c táº¡p gia tÄƒng.
- SEARCH-R1 tÆ°Æ¡ng thÃ­ch vá»›i nhiá»u thuáº­t toÃ¡n RL khÃ¡c nhau, bao gá»“m cáº£ **Proximal Policy Optimization (PPO)** vÃ  **Group Relative Policy Optimization (GRPO)**.
- **GRPO (Group Relative Policy Optimization)** lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a chÃ­nh sÃ¡ch khÃ¡c vá»›i PPO á»Ÿ chá»— nÃ³ **sá»­ dá»¥ng pháº§n thÆ°á»Ÿng trung bÃ¬nh cá»§a nhiá»u Ä‘áº§u ra Ä‘Æ°á»£c láº¥y máº«u lÃ m Ä‘Æ°á»ng cÆ¡ sá»Ÿ (baseline)** thay vÃ¬ dá»±a vÃ o má»™t hÃ m giÃ¡ trá»‹ (value function) Ä‘Æ°á»£c há»c. Äá»‘i vá»›i má»—i cÃ¢u há»i Ä‘áº§u vÃ o, GRPO láº¥y máº«u má»™t nhÃ³m pháº£n há»“i tá»« chÃ­nh sÃ¡ch tham kháº£o (reference policy) vÃ  sau Ä‘Ã³ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh chÃ­nh sÃ¡ch báº±ng cÃ¡ch tá»‘i Ä‘a hÃ³a má»™t hÃ m má»¥c tiÃªu dá»±a trÃªn pháº§n thÆ°á»Ÿng tÆ°Æ¡ng Ä‘á»‘i trong nhÃ³m.
- NghiÃªn cá»©u cho tháº¥y ráº±ng **GRPO thÆ°á»ng há»™i tá»¥ nhanh hÆ¡n PPO** vÃ¬ PPO dá»±a vÃ o má»™t mÃ´ hÃ¬nh phÃª bÃ¬nh (critic model) cáº§n má»™t sá»‘ bÆ°á»›c khá»Ÿi Ä‘á»™ng trÆ°á»›c khi quÃ¡ trÃ¬nh huáº¥n luyá»‡n hiá»‡u quáº£ báº¯t Ä‘áº§u. Tuy nhiÃªn, **PPO thá»ƒ hiá»‡n sá»± á»•n Ä‘á»‹nh huáº¥n luyá»‡n lá»›n hÆ¡n**, trong khi GRPO cÃ³ thá»ƒ dáº«n Ä‘áº¿n sá»± sá»¥p Ä‘á»• pháº§n thÆ°á»Ÿng trong má»™t sá»‘ trÆ°á»ng há»£p.
- Máº·c dÃ¹ cÃ³ sá»± khÃ¡c biá»‡t vá» tá»‘c Ä‘á»™ há»™i tá»¥ vÃ  Ä‘á»™ á»•n Ä‘á»‹nh, **pháº§n thÆ°á»Ÿng huáº¥n luyá»‡n cuá»‘i cÃ¹ng cá»§a PPO vÃ  GRPO lÃ  tÆ°Æ¡ng Ä‘Æ°Æ¡ng nhau**.
- Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ cho tháº¥y **GRPO thÆ°á»ng vÆ°á»£t trá»™i hÆ¡n PPO** trong viá»‡c tá»‘i Æ°u hÃ³a kháº£ nÄƒng suy luáº­n tÄƒng cÆ°á»ng báº±ng truy xuáº¥t. VÃ­ dá»¥, trÃªn cáº£ Qwen2.5-3B vÃ  LLaMA3.2-3B, GRPO Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t trung bÃ¬nh cao hÆ¡n.

## Training Templates

- As shown in Table 1, this template structures the modelâ€™s output into three parts in an iterative fashion: first, a reasoning process, then a search engine calling function, and finally, the answer.

## Reward modedling

- **rule-based** reward system that consists solely of final outcome rewards, which assess the **correctness of the modelâ€™s response**
- not use neural reward model cuz scared of reward hacking
- $r_\phi(x, y) = \text{EM}(a_{\text{pred}}, a_{\text{gold}})$,
- a_pred is the **extracted final answer** from response y and a_gold is the ground truth answer
    - How to extract a_pred from response y with rule-based?

## Experiment setup

- For retrieval, we use the 2018 Wikipedia dump (Karpukhin et al., 2020) as the knowledge source and E5 (Wang et al., 2022) as the retriever.
- follow Lin et al. (2023) and set the number of retrieved passages to three across all retrieval-based methods.  
- Exact Match (EM) is used as the evaluation metric, following Yu et al. (2024) (Rankrag: Unifying context ranking with retrieval-augmented generation in llms)
    - just check the source code lol i'm lazy to read paper ğŸ’€
    - WAIT WHAT? why outcome EM came from a RAG paper?

```python
def extract_solution(solution_str):
    """Extract the equation from the solution string."""
    # Remove everything before the first "Assistant:"
    # if "Assistant:" in solution_str:
    #     solution_str = solution_str.split("Assistant:", 1)[1]
    # elif "<|im_start|>assistant" in solution_str:
    #     solution_str = solution_str.split("<|im_start|>assistant", 1)[1]
    # else:
    #     return None
    # solution_str = solution_str.split('\n')[-1]

    answer_pattern = r'<answer>(.*?)</answer>'
    match = re.finditer(answer_pattern, solution_str, re.DOTALL)
    matches = list(match)
    
    # If there are 0 or exactly 1 matches, return None
    if len(matches) <= 1:
        return None
    
    # If there are 2 or more matches, return the last one
    return matches[-1].group(1).strip()


def compute_score_em(solution_str, ground_truth, method='strict', format_score=0., score=1.):
    """The scoring function for exact match (EM).

    Args:
        solution_str: the solution text
        ground_truth: the ground truth
        method: the method to extract the solution, choices are 'strict' and 'flexible'
        format_score: the score for the format
        score: the score for the correct answer
    """
    answer = extract_solution(solution_str=solution_str)
    do_print = random.randint(1, 64) == 1
    
    if do_print:
        print(f"--------------------------------")
        print(f"Golden answers: {ground_truth['target']}")
        print(f"Extracted answer: {answer}")
        print(f"Solution string: {solution_str}")
    
    if answer is None:
        return 0
    else:
        if em_check(answer, ground_truth['target']):
            return score
        else:
            return format_score
```

## Datasets

- Training dataset:merge  training sets of NQ and HotpotQA
- Seven **benchmark**datasets,
    - General Question Answering: NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2022).
    - Multi-Hop Question Answering: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), Musique (Trivedi et al., 2022b), and Bamboogle (Press et al., 2022).

## Evaluation Baselines

- Inference without Retrieval: Direct inference and Chain-of-Thought (CoT) reasoning
- Inference with Retrieval: Retrieval-Augmented Generation (RAG)
- Finetune base models It only contains reasoning and answer steps and cannot call a search engine.

## Hotpot QA and NQ

### HotpotQA

- Size: ~113K crowd-sourced questions
- Type: Multi-hop question answering dataset
- Source: English Wikipedia
- Key Features:
    - Requires reading 2 Wikipedia articles to answer each question
    - Comes with gold paragraphs and supporting facts identified by crowdworkers
    - Diverse reasoning strategies including:
        - Questions with missing entities
        - Intersection questions ("What satisfies property A and B?")
        - Comparison questions (comparing entities by common attributes)

Two settings:

1. Few-document distractor: Models get 10 paragraphs containing the gold paragraphs
2. Open-domain fullwiki: Models only get the question and access to Wikipedia

Evaluation metrics:

- Answer accuracy: Exact Match (EM) and unigram F1
- Explainability: Supporting Fact EM/F1
- Joint metric for both tasks

### Natural Questions (NQ)

- Size: 300,000 questions
- Type: Open-domain question answering
- Source: Real Google search queries
- Key Features:
    - Natural questions from real users
    - Human-annotated answers from Wikipedia pages
    - Additional 16,000 examples with 5 different annotators per question for evaluation
    - Replicates end-to-end process of how people find answers

Example from HotpotQA (comparison type):

```
Question: "Which magazine was started first Arthur's Magazine or First for Women?"
Supporting Facts: 
- Arthur's Magazine was a literary periodical established in 1844
- First for Women is a woman's magazine launched in 1989
Answer: "Arthur's Magazine"
```
